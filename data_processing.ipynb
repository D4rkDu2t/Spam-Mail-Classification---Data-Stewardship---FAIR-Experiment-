{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input preprocessing - Spam Email Detection using Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/saito/Desktop/TU WIEN - SS 2025/Data stewardship/Ex2/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import stanza\n",
    "import string\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "      <th>label_num</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id label                                               text  label_num\n",
       "0   605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...          0\n",
       "1  2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...          0\n",
       "2  3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...          0\n",
       "3  4685  spam  Subject: photoshop , windows , office . cheap ...          1\n",
       "4  2030   ham  Subject: re : indian springs\\r\\nthis deal is t...          0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import original data\n",
    "df = pd.read_csv(\"data/spam_ham_dataset.csv\")\n",
    "\n",
    "# Rename id and text columns\n",
    "df.rename(columns={\n",
    "    'Unnamed: 0': 'id',\n",
    "    'text': 'original_text'\n",
    "}, inplace=True)\n",
    "\n",
    "# Print first 5 elements\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characters removal\n",
    "\n",
    "Remove all words that are considered as stop words in the ntlk library. This includes common words such as \"the\", \"is\", \"in\", etc. that do not add significant meaning to the text; also remove punctuation. This is done to standardize the text and make it easier to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "punctuation = list(string.punctuation)\n",
    "\n",
    "remove = stop_words + punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemmatized_text'] = df['original_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (remove)]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "Lemmatization is the process of reducing words to their base or root form. For example, \"running\" becomes \"run\", and \"better\" becomes \"good\". This helps in reducing the dimensionality of the data and improving the performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-13 15:31:37 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json: 426kB [00:00, 218MB/s]                     \n",
      "2025-04-13 15:31:37 INFO: Downloaded file to /Users/saito/stanza_resources/resources.json\n",
      "2025-04-13 15:31:37 WARNING: Language en package default expects mwt, which has been added\n",
      "2025-04-13 15:31:38 INFO: Loading these models for language: en (English):\n",
      "=================================\n",
      "| Processor | Package           |\n",
      "---------------------------------\n",
      "| tokenize  | combined          |\n",
      "| mwt       | combined          |\n",
      "| pos       | combined_charlm   |\n",
      "| lemma     | combined_nocharlm |\n",
      "=================================\n",
      "\n",
      "2025-04-13 15:31:38 INFO: Using device: cpu\n",
      "2025-04-13 15:31:38 INFO: Loading: tokenize\n",
      "2025-04-13 15:31:38 INFO: Loading: mwt\n",
      "2025-04-13 15:31:38 INFO: Loading: pos\n",
      "2025-04-13 15:31:39 INFO: Loading: lemma\n",
      "2025-04-13 15:31:39 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Create a Stanza pipeline for the English language\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,lemma,pos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>original_text</th>\n",
       "      <th>label_num</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>605</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: enron methanol ; meter # : 988291\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject : enron methanol meter 988291 follow n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2349</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: hpl nom for january 9 , 2001\\r\\n( see...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject : hpl nom january 9 2001 see attach fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3624</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: neon retreat\\r\\nho ho ho , we ' re ar...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject : neon retreat ho ho ho around wonderf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4685</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: photoshop , windows , office . cheap ...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject : photoshop window office cheap main t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2030</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: re : indian springs\\r\\nthis deal is t...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject : indian spring deal book teco pvr rev...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5166</th>\n",
       "      <td>1518</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: put the 10 on the ft\\r\\nthe transport...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject : put 10 ft transport volume decrease ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5167</th>\n",
       "      <td>404</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: 3 / 4 / 2000 and following noms\\r\\nhp...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject : 3 4 2000 follow nom hpl take extra 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5168</th>\n",
       "      <td>2933</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: calpine daily gas nomination\\r\\n&gt;\\r\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject : calpine daily gas nomination julie m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5169</th>\n",
       "      <td>1409</td>\n",
       "      <td>ham</td>\n",
       "      <td>Subject: industrial worksheets for august 2000...</td>\n",
       "      <td>0</td>\n",
       "      <td>subject : industrial worksheet august 2000 act...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5170</th>\n",
       "      <td>4807</td>\n",
       "      <td>spam</td>\n",
       "      <td>Subject: important online banking alert\\r\\ndea...</td>\n",
       "      <td>1</td>\n",
       "      <td>subject : important online banking alert dear ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5171 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id label                                      original_text  \\\n",
       "0      605   ham  Subject: enron methanol ; meter # : 988291\\r\\n...   \n",
       "1     2349   ham  Subject: hpl nom for january 9 , 2001\\r\\n( see...   \n",
       "2     3624   ham  Subject: neon retreat\\r\\nho ho ho , we ' re ar...   \n",
       "3     4685  spam  Subject: photoshop , windows , office . cheap ...   \n",
       "4     2030   ham  Subject: re : indian springs\\r\\nthis deal is t...   \n",
       "...    ...   ...                                                ...   \n",
       "5166  1518   ham  Subject: put the 10 on the ft\\r\\nthe transport...   \n",
       "5167   404   ham  Subject: 3 / 4 / 2000 and following noms\\r\\nhp...   \n",
       "5168  2933   ham  Subject: calpine daily gas nomination\\r\\n>\\r\\n...   \n",
       "5169  1409   ham  Subject: industrial worksheets for august 2000...   \n",
       "5170  4807  spam  Subject: important online banking alert\\r\\ndea...   \n",
       "\n",
       "      label_num                                    lemmatized_text  \n",
       "0             0  subject : enron methanol meter 988291 follow n...  \n",
       "1             0  subject : hpl nom january 9 2001 see attach fi...  \n",
       "2             0  subject : neon retreat ho ho ho around wonderf...  \n",
       "3             1  subject : photoshop window office cheap main t...  \n",
       "4             0  subject : indian spring deal book teco pvr rev...  \n",
       "...         ...                                                ...  \n",
       "5166          0  subject : put 10 ft transport volume decrease ...  \n",
       "5167          0  subject : 3 4 2000 follow nom hpl take extra 1...  \n",
       "5168          0  subject : calpine daily gas nomination julie m...  \n",
       "5169          0  subject : industrial worksheet august 2000 act...  \n",
       "5170          1  subject : important online banking alert dear ...  \n",
       "\n",
       "[5171 rows x 5 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def apply_lemmatization(nlp, df):\n",
    "    def process_row(row):\n",
    "        doc = nlp(row[\"lemmatized_text\"])\n",
    "        lemmatized_text = join_comment(doc)\n",
    "        return lemmatized_text\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        lemmatized_texts = list(executor.map(process_row, [row for _, row in df.iterrows()]))\n",
    "\n",
    "    df[\"lemmatized_text\"] = lemmatized_texts\n",
    "    return df\n",
    "\n",
    "def join_comment(doc):\n",
    "    # Iterate over the sentences in the doc and then over the tokens in each sentence\n",
    "    lemmatized_words = []\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for token in sentence.tokens:\n",
    "            lemmatized_words.append(token.words[0].lemma)\n",
    "    \n",
    "    return \" \".join(lemmatized_words)\n",
    "\n",
    "\n",
    "new_df = apply_lemmatization(nlp, df)\n",
    "\n",
    "new_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data splitting\n",
    "\n",
    "The dataset will be split into training, validation and test sets. The training set will be used to train the model, the validation set will be used to tune the hyperparameters, and the test set will be used to evaluate the final model's performance.\n",
    "\n",
    "The data will be randomly split (with the seed 42) while maintaining the same distribution of classes in each set, using stratified sampling from the `train_test_split` function from  `sklearn`.\n",
    "\n",
    "The training set will contain 70% of the data, the validation set will contain 10% of the data, and the test set will contain 20% of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split new_df to 70% train_set, 30% temporal set (training + validation)\n",
    "train_set, temporal_set  = train_test_split(new_df, test_size=0.3, random_state=42)\n",
    "\n",
    "# Split temporal_set to 67% test_set, 33% validation_set\n",
    "test_set, validation_set  = train_test_split(temporal_set, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to split the sets easily on DBREPO, we created a new id called `experiment_id`. This id is used to identify each subset by a range of values. For example, the training set will have `experiment_id` values from 0 to 3618, the validation set will have `experiment_id` values from 3619 to 4131, and the test set will have `experiment_id` values from 4132 to 5170. This while conserving the orginal id of the dataset. This is done to make it easier to track the subsets and their corresponding original ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign experiment id\n",
    "# Experiment id is new consecutive number for each comment staring from 0 from train_set and counting up to the last comment in test_set\n",
    "train_set['experiment_id'] = np.arange(len(train_set))\n",
    "validation_set['experiment_id'] = np.arange(len(train_set), len(train_set) + len(validation_set))\n",
    "test_set['experiment_id'] = np.arange(len(train_set) + len(validation_set), len(train_set) + len(validation_set) + len(test_set))\n",
    "\n",
    "# Reorder and filter columns\n",
    "final_train_set = train_set[['id', 'experiment_id', 'lemmatized_text', 'label', 'label_num']]\n",
    "final_validation_set = validation_set[['id', 'experiment_id', 'lemmatized_text', 'label', 'label_num']]\n",
    "final_test_set = test_set[['id', 'experiment_id', 'lemmatized_text', 'label', 'label_num']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sets into csv\n",
    "final_train_set.to_csv('data/train_set.csv', index=False)\n",
    "final_test_set.to_csv('data/test_set.csv', index=False)\n",
    "final_validation_set.to_csv('data/validation_set.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
